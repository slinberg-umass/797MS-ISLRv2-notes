---
title: "ISLRv2 chapter 2 exercises"
author: "Steve Linberg, DACSS 797MS Spring 2022"
date: "2022-01-13"
# output: html_document
output:
  # tufte::tufte_html:
  #     tufte_variant: "envisioned"
  tufte::tufte_html: default
  tufte::tufte_handout: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(tufte)
```

# For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

`r newthought('Terminology note: the terms')` *flexible* and *inflexible* don't appear to be formally defined, and most of the external searches I conducted for these terms in the context of statistical modeling simply led back either to the [textbook's website](https://www.statlearning.com/) itself, or to [discussions of the textbook](https://stats.stackexchange.com/questions/69237/flexible-and-inflexible-models-in-machine-learning), Neither term is used in the book beyond chapter 2, where they are introduced, and every use of *inflexible* is within the cautionary phrase *"relatively inflexible"* in the context of linear regression and least-square methods.

It appears, therefore, that they are general-purpose terms that reflect, literally, a model's flexibility, which is a measure of how many parameters it can incorporate. One outcome of this is that the term *inflexible* is a close synonym of *simple* or *linear*, as in a simple linear regression that only has one parameter, and that a *flexible* method or model would be exemplified by a multiple linear regression incorporating many parameters, creating a curve with more complexity.

The very general trend to observe in this context is that *inflexible*, or *simple*, models may run the risk of not reflecting the true complexity of the underlying data (and therefore being *biased*), where *flexible* models may risk *overfitting* the data and performing poorly when applied to new data (and thus having high *variance*).

## (a) The sample size n is extremely large, and the number of predictors p is small.

The text does not appear to directly state anything concrete regarding this and the following examples. I suspect that the answer being sought here is that a flexible model is better for models with very large numbers of observations, and this may be because a larger *n* may be likely to be less linear in a very broad generalized sense.

Note, however, on p.22:

> in general, fitting a more flexible model requires estimating a greater number of parameters

saying that flexible models generally require higher numbers of predictors, and p.23:

> non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.

This implies that non-parametric methods (which are more flexible than parametric methods) tend to have larger numbers of parameters, which somewhat contradicts the above conclusion. And while it says that non-parametric methods may be *required* for situations with large numbers of observations, it does not say that they are *preferable*. It seems to me that it would depend to some extent on how much variability is found in the data. A very large dataset with strong uniformity (i.e. with a highly linear true $f$) could be served very well by an inflexible model.

(All of this assumes that *non-parametric* largely overlaps, if not being synonymous with, *flexible*.)

## (b) The number of predictors p is extremely large, and the number of observations n is small.

See above; again, I suspect the answer being sought is that a less flexible, more linear model would be better for cases with small number of observations, but data with high variability might still be better served by a flexible model with less bias.

## (c) The relationship between the predictors and response is highly non-linear.

If this is describing a situation with a high degree of noise in the data, or a high irreducible error rate, a more flexible model will be better able to follow the data (subject to the ubiquitous risk of overfitting). However, the text suggests that this should be coupled with a large number of observations (p.36):

> if the true $f$ is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach

This implies that if the relationship between the predictors and response is highly non-linear, but the number of observations is small, a less flexible method may be preferable.

## (d) The variance of the error terms, i.e. $\sigma^2 = Var(\epsilon)$, is extremely high.

Somewhat following from the above answer; the larger the variance of the error terms, the likelier it is that overfitting would occur as the noise is chased. A less flexible approach will have more bias but less variance.

# 2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.

```{marginfigure}
^["The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate *predictions* possible. Statistical models are designed for *inference* about the relationships between variables." - Matthew Stewart, [The Actual Difference Between Statistics and Machine Learning](https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3)
```

(*prediction* is trying to anticipate the value, or likely outcome, of a set of predictors; *inference* is trying to understand the relationship between the outcome and the predictors. Both fall under the category of supervised learning, since there *is* an outcome variable to predict. *Regression* problems have quantitative outcome variables; *classification* problems have categorical outcome variables. $n$ is the sample size, or number of observations, and $p$ is the number of predictors.)

## (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

This is an inference problem, since we are trying to understand the relationship between the predictors and the salary. It is a regression problem, since the outcome variable (salary) is quantitative. $n$ in this is case is 500 (for the firms), and $p$ is 4 (profit, number of employees, industry and salary).

## (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

This is a prediction problem, since the goal is to know the outcome but not necessarily care about the relationships between the predictors and the outcome. Since the outcome is success or failure, it is a classification problem. $n$ is 20, for the other products being examined, and $p$ is 14 (success/failure, price, marketing budget, competition price, and "ten other variables").

## (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

This is a prediction problem, as clearly described, and since the outcome variable is quantitative (% change), it is a regression problem. $n$ is 52 for 52 weeks of data in 2012, and $p$ is 4 for the percentage changes enumerated.

# 3. We now revisit the bias-variance decomposition.

## (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

## (b) Explain why each of the five curves has the shape displayed in part (a).

# 4. You will now think of some real-life applications for statistical learning.

## (a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

1. Does a person have COVID-19? The response would be a yes/no; the predictors would be hard to guess without a wealth of medical knowledge, but they would be whatever the tests are measuring. This would be a prediction problem, since the task is to predict whether someone is infected given the presence and levels of various medical measures.
2. What is a person's risk level for developing diabetes? There would be no fixed set of responses for this, but could (for example) be "low", "medium" and "high", based on various input parameters such as test results and lifestyle factors. This would also be a prediction problem, since the task would be to measure a risk of an unknown future situation.
3. Is an email spam or ham? The outcome would be a yes/no (though spam-detection algorithms usually have a numeric metric with a threshold for classification). The predictors would be various aspects of the email, including its content, subject, sender, and other metadata that could affect the spam weights. 

## (b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.


## (c) Describe three real-life applications in which cluster analysis might be useful.

# 5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

Flexible approaches can model more complex relationships with models containing many predictors, and can theoretically come up with more accurate predictions in contexts where a less flexible model has high bias. The risk is that a very flexible model can overfit the data and end up with high variance when tested against different data that it was trained on. Less flexible models also have the advantage of simplicity and comprehensibility, which may be preferred over accuracy in certain contexts.

# 6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

Parametric models seek to estimate $f$ through analysis of parameters (variables), so that future/unknown $Y$ values can be predicted based on these parameters, and the relationship between the predictors and the outcome can be estimated, quantified and analyzed. Non-parametric models do not try to estimate $f$, but instead try to provide a mechanism to predict the training data as closely as possible without investigating the relationships between the parameters. Parametric approaches can provide useful simplicity to understanding the relationships between predictors, but run the risk of oversimplifying or missing important complexity.

# 7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

|Obs.|$X_1$|$X_2$|$X_3$|$Y$|
|---|--:|--:|--:|:-:|
|1|0|3|0|Red|
|2|2|0|0|Red|
|3|0|1|3|Red|
|4|0|1|2|Green|
|5|−1|0|1|Green|
|6|1|1|1|Red|

Suppose we wish to use this data set to make a prediction for Y when $X_1 = X_2 = X_3 = 0$ using K-nearest neighbors.

## (a) Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.
## (b) What is our prediction with $K = 1$? Why?
## (c) What is our prediction with $K = 3$? Why?
## (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?

# 8. This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US. The variables are

- `Private` : Public/private indicator
- `Apps` : Number of applications received
- `Accept` : Number of applicants accepted
- `Enroll` : Number of new students enrolled
- `Top10perc` : New students from top 10 % of high school class
- `Top25perc` : New students from top 25 % of high school class
- `F.Undergrad` : Number of full-time undergraduates
- `P.Undergrad` : Number of part-time undergraduates
- `Outstate` : Out-of-state tuition
- `Room.Board` : Room and board costs
- `Books` : Estimated book costs
- `Personal` : Estimated personal spending
- `PhD` : Percent of faculty with Ph.D.’s
- `Terminal` : Percent of faculty with terminal degree
- `S.F.Ratio` : Student/faculty ratio
- `perc.alumni` : Percent of alumni who donate
- `Expend` : Instructional expenditure per student
- `Grad.Rate` : Graduation rate

Before reading the data into R, it can be viewed in Excel or a text editor.

(a) (b)
## (a) Use the `read.csv()` function to read the data into `R`. Call the loaded data `college`. Make sure that you have the directory set to the correct location for the data.

```{r}
college <- read.csv("College.csv")
```

## (b) Look at the data using the `View()` function. You should notice that the first column is just the name of each university. We don’t really want `R` to treat this as data. However, it may be handy to have these names for later. Try the following commands:

```{r}
rownames(college) <- college[, 1] 
# View(college)
```

You should see that there is now a `row.names` column with the name of each university recorded. This means that `R` has given each row a name corresponding to the appropriate university. `R` will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try

```{r}
college <- college[, -1] 
# View(college)
```

Now you should see that the first data column is `Private.` Note that another column labeled `row.names` now appears before the `Private` column. However, this is not a data column but rather the name that `R` is giving to each row.

## (c) i. Use the `summary()` function to produce a numerical summary of the variables in the data set.

```{r}
summary(college)
```

## ii. Use the `pairs()` function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix `A` using `A[,1:10]`.

Note: `pairs` cannot be used for character vectors or columns; `Private`, the first column, is a character vector. The first 10 numeric columns would be columns 2 through 11, rather than 1 through 10.

```{r}
pairs(college[,2:11])
```

## iii. Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private.`

```{r}
plot(as.factor(college$Private), college$Outstate, xlab = "Private", ylab = "Outstate tuition")
```

## iv. Create a new qualitative variable, called `Elite`, by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50 %.

(Let's be a little more modern with this than the supplied code.)

```{r}
suppressPackageStartupMessages({
  library(dplyr)
})
college <- college %>%
  mutate(college, Elite = as.factor(ifelse(Top10perc > 50, "Yes", "No")))
```

## Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite.`

```{r}
summary(college$Elite)
plot(college$Elite, college$Outstate, xlab = "Elite", ylab = "Outstate tuition")
```

## v. Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow = c(2, 2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.

```{r}
par(mfrow = c(2, 2))
hist(college$Apps)
hist(college$Accept)
hist(college$Enroll)
hist(college$Top10perc)
```

vi. Continue exploring the data, and provide a brief summary of what you discover.

# 9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.

```{r}
Auto <- read.table("Auto.data", header = T, na.strings = "?", stringsAsFactors = T)
```

## (a) Which of the predictors are quantitative, and which are qualitative?

mpg, cylinders, displacement, horsepower, weight and acceleration are quantitative. year, origin and name are qualitative.