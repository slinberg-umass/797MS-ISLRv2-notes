---
title: "ISLRv2 Glossary"
author: "Steve Linberg, DACSS 797MS Spring 2022"
date: "2022"
output:
  tufte::tufte_html: 
      tufte_variant: "envisioned"
  tufte::tufte_handout: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(tufte)
```

Page numbers in parenthesis after terms.

input variable (15)
: also *predictor*, *independent variable*, *feature*; usually written $X_1, X_2$, etc. The parameter or parameters we are testing to see if they are related to or affect the output.

output variable (15)
: also *response*, *dependent variable*; usually written $Y$. The outcome being measured.

error term (16)
: $\epsilon$ in the equation  
$$Y = f(X) + \epsilon$$
a random quantity of inaccuracy, *independent of X* and with *mean 0*.

systematic (16)
: $f$ in the equation  
$$Y = f(X) + \epsilon$$
the function that describes the (systematic) information $X$ provides about $Y$. This plus the error term equals $Y$.

reducible error (18)
: The amount of the error $\epsilon$ that could be eliminated by improving our estimator $\hat{f}$; the difference between $\hat{f}$ and $f$. This book and course is mostly about ways to minimize the reducible error.

irreducible error (18)
: The amount of $\epsilon$ that could not be reduced even if $f$ was a perfect estimator of $Y$. Always greater than 0. Could be due to hidden variables in $\epsilon$, or random fluctuations in Y, like a measure of "[a] patient's general feeling of well-being on that day".

expected value (19)
: *average value* of an expected measure.

training data (21)
: data used to develop the model for estimating $f$.

parametric methods (21)
: A model based on one or more input parameters, that yields a value for Y, as in:
$$f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p$$
$$Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots + \beta_pX_p$$
$$\text{income} \approx \beta_0 + \beta_1 \times \text{education} + \beta_2 \times \text{seniority}$$
This creates a predictive, *inflexible* model which usually does not match the true $f$, but which has advantages of simplicity and interpretability. It can be used to predict values for $Y$ based on its parameters, or inputs. Linear and logistic regression are parametric.

non-parametric methods (23)
: methods that do not attempt to estimate $f$. More flexible and have the potential to very closely match observations, but with the risk of *overfitting* the data and increasing the *variance* of subsequent observations. They require much more data than parametric models, and may be difficult to interpret, K-Nearest Neighbor and Support Vector Machines are non-parametric.

prediction (26)
: seeking to guess the value of an response variable $y_i$ given a set of observations and a predictor $f$.

inference (26)
: a model that seems to better understand the relationship between the response and the predictors.

supervised learning (26)
: a category of model that allows us to guess a $y_i$ response to a set of predictor measurements $x_i, i = 1, \dots, n$.

unsupervised learning (26)
: a category of model in which there are observations/measurements $x_i, i = 1, \dots, n$, but no associated response $y_i$. Linear regression cannot be used because there is no response variable to predict.

quantitative variables (28)
: numeric values; age, height, weight, quantity. Usually the response variable type for regression problems.

qualitative variables (28)
: also *categorical*: values from a discrete set. Eye color, name, yes/no. Usually the response variable type for classification problems.

regression problems (28)
: problems with quantitative response variables.

classification problems (28)
: problems with qualitative response variables.

mean squared error (MSE) (29)
: the average squared error for a set of observations:
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2$$
MSE is small if the predicted responses are close to the true responses, and larger as it becomes less accurate; computed from training data, and Gareth *et al.* suggest it should be called *training MSE*.

variance (34)
: _"the amount by which $\hat{f}$ would change if we estimated it using a different training data set"_

bias (35)
: _"the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model"_, as in the error from the (presumed) linearity of a regression against non-linear data whose complexity it does not capture. More flexible models increase variance and decrease bias.